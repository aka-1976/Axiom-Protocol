/// Neural Guardian - Federated Learning AI Security System
/// 
/// This module implements a decentralized AI-powered network security system
/// that trains collaboratively across nodes without sharing raw data.

use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use sha2::{Digest, Sha256};

/// Network event for training the Neural Guardian
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct NetworkEvent {
    pub peer_id: String,
    pub block_interval: f32,      // Time between blocks (seconds)
    pub block_size: f32,          // Block size in KB
    pub tx_count: f32,            // Transactions per block
    pub propagation_time: f32,    // Time to receive block (ms)
    pub peer_count: f32,          // Number of active peers
    pub fork_count: f32,          // Number of forks observed
    pub orphan_rate: f32,         // Orphaned blocks ratio
    pub reorg_depth: f32,         // Reorganization depth
    pub bandwidth_usage: f32,     // Network bandwidth (KB/s)
    pub connection_churn: f32,    // Peer connect/disconnect rate
    pub timestamp: u64,
}

/// Threat types that Neural Guardian can detect
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub enum ThreatType {
    SelfishMining,     // Miner withholds blocks
    SybilAttack,       // Fake peer identities
    EclipseAttack,     // Network isolation
    DoS,               // Denial of service
    TimestampManip,    // VDF timing manipulation
    Benign,            // No threat detected
}

/// Threat assessment result
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ThreatAssessment {
    pub peer_id: String,
    pub trust_score: f32,         // 0.0 (untrusted) to 1.0 (trusted)
    pub detected_threats: Vec<ThreatType>,
    pub confidence: f32,          // Model confidence
    pub recommended_action: Action,
}

/// Recommended actions based on threat detection
#[derive(Clone, Debug, PartialEq, Serialize, Deserialize)]
pub enum Action {
    None,
    IncreaseMonitoring,
    LimitConnections,
    DiversifyPeers,
    RateLimit,
    VerifyVDF,
    BanPeer,
}

/// Simple neural network for threat detection
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct NeuralNetwork {
    // Input layer (10 features) -> Hidden layer (64) -> Output layer (6 threat types)
    weights_input_hidden: Vec<Vec<f32>>,  // 10x64
    bias_hidden: Vec<f32>,                // 64
    weights_hidden_output: Vec<Vec<f32>>, // 64x6
    bias_output: Vec<f32>,                // 6
}

impl NeuralNetwork {
    /// Create a new neural network with random initialization
    pub fn new() -> Self {
        use rand::Rng;
        let mut rng = rand::thread_rng();
        
        let input_size = 10;
        let hidden_size = 64;
        let output_size = 6; // 6 threat types (including Benign)
        
        // Xavier initialization
        let weights_input_hidden: Vec<Vec<f32>> = (0..input_size)
            .map(|_| {
                (0..hidden_size)
                    .map(|_| rng.gen_range(-1.0..1.0) * (2.0 / input_size as f32).sqrt())
                    .collect()
            })
            .collect();
        
        let bias_hidden: Vec<f32> = (0..hidden_size).map(|_| 0.0).collect();
        
        let weights_hidden_output: Vec<Vec<f32>> = (0..hidden_size)
            .map(|_| {
                (0..output_size)
                    .map(|_| rng.gen_range(-1.0..1.0) * (2.0 / hidden_size as f32).sqrt())
                    .collect()
            })
            .collect();
        
        let bias_output: Vec<f32> = (0..output_size).map(|_| 0.0).collect();
        
        Self {
            weights_input_hidden,
            bias_hidden,
            weights_hidden_output,
            bias_output,
        }
    }
    
    /// Forward pass through the network
    pub fn forward(&self, input: &[f32]) -> Vec<f32> {
        // Input to hidden layer
        let mut hidden: Vec<f32> = self.bias_hidden.clone();
        for (i, h) in hidden.iter_mut().enumerate() {
            for (j, &inp) in input.iter().enumerate() {
                *h += inp * self.weights_input_hidden[j][i];
            }
            *h = relu(*h); // ReLU activation
        }
        
        // Hidden to output layer
        let mut output: Vec<f32> = self.bias_output.clone();
        for (i, o) in output.iter_mut().enumerate() {
            for (j, &h) in hidden.iter().enumerate() {
                *o += h * self.weights_hidden_output[j][i];
            }
        }
        
        // Softmax activation
        softmax(&output)
    }
    
    /// Single-layer gradient descent training step with full backpropagation.
    ///
    /// Computes output-layer deltas, then propagates gradients back to
    /// the input‚Üíhidden weights using the chain rule.
    pub fn train_step(&mut self, input: &[f32], target: &[f32], learning_rate: f32) {
        // Forward pass ‚Äî compute hidden activations for backprop
        let mut hidden: Vec<f32> = self.bias_hidden.clone();
        for (i, h) in hidden.iter_mut().enumerate() {
            for (j, &inp) in input.iter().enumerate() {
                *h += inp * self.weights_input_hidden[j][i];
            }
            *h = relu(*h);
        }
        let prediction = self.forward(input);

        // Output-layer deltas
        let output_deltas: Vec<f32> = target.iter().zip(prediction.iter())
            .map(|(&t, &p)| t - p)
            .collect();

        // Update hidden‚Üíoutput weights
        for i in 0..self.weights_hidden_output.len() {
            for j in 0..self.weights_hidden_output[i].len() {
                self.weights_hidden_output[i][j] += learning_rate * output_deltas[j] * hidden[i];
            }
        }

        // Backpropagate to input‚Üíhidden weights
        for j in 0..self.weights_input_hidden.len() {
            for i in 0..self.weights_input_hidden[j].len() {
                // Gradient through ReLU: only flows if hidden[i] > 0
                if hidden[i] > 0.0 {
                    let grad: f32 = output_deltas.iter().enumerate()
                        .map(|(k, &d)| d * self.weights_hidden_output[i][k])
                        .sum();
                    self.weights_input_hidden[j][i] += learning_rate * grad * input[j];
                }
            }
        }
    }
}

/// ReLU activation function
fn relu(x: f32) -> f32 {
    if x > 0.0 { x } else { 0.0 }
}

/// Softmax activation for output layer
fn softmax(values: &[f32]) -> Vec<f32> {
    let max = values.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
    let exp_values: Vec<f32> = values.iter().map(|&v| (v - max).exp()).collect();
    let sum: f32 = exp_values.iter().sum();
    exp_values.iter().map(|&v| v / sum).collect()
}

/// Model update for federated learning
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct ModelUpdate {
    pub node_id: String,
    pub gradients_hash: [u8; 32],
    pub num_samples: usize,
    pub loss: f32,
    pub timestamp: u64,
}

/// Neural Guardian with federated learning
pub struct NeuralGuardian {
    model: NeuralNetwork,
    peer_history: HashMap<String, Vec<NetworkEvent>>,
    threat_cache: HashMap<String, ThreatAssessment>,
    training_data: Vec<(NetworkEvent, ThreatType)>,
    /// SHA-256 hex digest of the currently loaded model weights.
    /// Set by [`load_model`] on startup; defaults to the hash of the
    /// freshly-initialised random weights.
    model_hash: String,
}

impl Default for NeuralGuardian {
    fn default() -> Self {
        Self::new()
    }
}

impl NeuralGuardian {
    pub fn new() -> Self {
        let model = NeuralNetwork::new();
        // Compute hash of the freshly-initialised model weights
        let model_hash = Self::hash_model_weights(&model);
        Self {
            model,
            peer_history: HashMap::new(),
            threat_cache: HashMap::new(),
            training_data: Vec::new(),
            model_hash,
        }
    }

    /// SHA-256 hex digest of the model's weight matrices.
    fn hash_model_weights(model: &NeuralNetwork) -> String {
        let mut hasher = Sha256::new();
        for row in &model.weights_input_hidden {
            for &w in row {
                hasher.update(w.to_le_bytes());
            }
        }
        for &b in &model.bias_hidden {
            hasher.update(b.to_le_bytes());
        }
        for row in &model.weights_hidden_output {
            for &w in row {
                hasher.update(w.to_le_bytes());
            }
        }
        for &b in &model.bias_output {
            hasher.update(b.to_le_bytes());
        }
        hex::encode(hasher.finalize())
    }

    /// Load model weights from a file and verify integrity against the
    /// genesis anchor [`crate::GENESIS_WEIGHTS_HASH`].
    ///
    /// The node **must** call this at startup.  If the SHA-256 hash of
    /// the file does not match the genesis constant, the node panics
    /// with a clear integrity failure message.
    pub fn load_model(&mut self, path: std::path::PathBuf) -> Result<(), String> {
        let data = std::fs::read(&path).map_err(|e| {
            format!("Failed to read model weights from {}: {}", path.display(), e)
        })?;

        let mut hasher = Sha256::new();
        hasher.update(&data);
        let file_hash = hex::encode(hasher.finalize());

        let expected = crate::GENESIS_WEIGHTS_HASH;

        assert_eq!(
            file_hash, expected,
            "INTEGRITY FAILURE: Model weights do not match Genesis Anchor. \
             Auditability compromised. (got {}, expected {})",
            file_hash, expected
        );

        // Deserialize and install the verified weights
        let model: NeuralNetwork = bincode::deserialize(&data).map_err(|e| {
            format!("Failed to deserialize model weights: {}", e)
        })?;
        self.model = model;
        self.model_hash = file_hash;

        Ok(())
    }
    
    /// Extract features from network event
    pub fn extract_features(&self, event: &NetworkEvent) -> Vec<f32> {
        vec![
            normalize_time(event.block_interval),
            normalize_size(event.block_size),
            normalize_count(event.tx_count),
            normalize_time(event.propagation_time),
            normalize_count(event.peer_count),
            normalize_count(event.fork_count),
            event.orphan_rate,
            normalize_depth(event.reorg_depth),
            normalize_size(event.bandwidth_usage),
            normalize_rate(event.connection_churn),
        ]
    }
    
    /// Analyze peer and detect threats
    pub fn analyze_peer(&mut self, peer_id: &str) -> Option<ThreatAssessment> {
        // Check cache first
        if let Some(cached) = self.threat_cache.get(peer_id) {
            return Some(cached.clone());
        }
        
        // Get peer history
        let events = self.peer_history.get(peer_id)?;
        if events.is_empty() {
            return None;
        }
        
        // Extract features from recent events
        let recent_event = &events[events.len() - 1];
        let features = self.extract_features(recent_event);
        
        // Run through model
        let predictions = self.model.forward(&features);
        
        // Interpret predictions (indices correspond to ThreatType variants)
        let selfish_mining_prob = predictions[0];
        let sybil_prob = predictions[1];
        let eclipse_prob = predictions[2];
        let dos_prob = predictions[3];
        let timestamp_prob = predictions[4];
        let benign_prob = predictions[5];
        
        let mut threats = Vec::new();
        if selfish_mining_prob > 0.7 {
            threats.push(ThreatType::SelfishMining);
        }
        if sybil_prob > 0.8 {
            threats.push(ThreatType::SybilAttack);
        }
        if eclipse_prob > 0.6 {
            threats.push(ThreatType::EclipseAttack);
        }
        if dos_prob > 0.7 {
            threats.push(ThreatType::DoS);
        }
        if timestamp_prob > 0.6 {
            threats.push(ThreatType::TimestampManip);
        }
        
        let max_threat_prob = predictions.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let trust_score = 1.0 - max_threat_prob;
        
        let assessment = ThreatAssessment {
            peer_id: peer_id.to_string(),
            trust_score,
            detected_threats: threats.clone(),
            confidence: benign_prob,
            recommended_action: determine_action(&threats),
        };
        
        // Cache the assessment
        self.threat_cache.insert(peer_id.to_string(), assessment.clone());
        
        Some(assessment)
    }
    
    /// Record a network event for a peer
    pub fn record_event(&mut self, peer_id: String, event: NetworkEvent) {
        self.peer_history
            .entry(peer_id)
            .or_default()
            .push(event);
    }
    
    /// Train the model on local data
    pub fn train_local(&mut self, epochs: u32, learning_rate: f32) -> ModelUpdate {
        let mut total_loss = 0.0;
        
        for _ in 0..epochs {
            for (event, threat) in &self.training_data {
                let features = self.extract_features(event);
                let target = threat_to_one_hot(threat);
                
                self.model.train_step(&features, &target, learning_rate);
                
                // Compute loss (cross-entropy)
                let prediction = self.model.forward(&features);
                let loss: f32 = target
                    .iter()
                    .zip(prediction.iter())
                    .map(|(&t, &p)| -t * p.max(1e-10).ln())
                    .sum();
                total_loss += loss;
            }
        }
        
        let avg_loss = total_loss / (epochs as f32 * self.training_data.len() as f32);
        
        // Compute gradients hash for verification
        let gradients_hash = self.compute_gradients_hash();
        
        ModelUpdate {
            node_id: "local".to_string(),
            gradients_hash,
            num_samples: self.training_data.len(),
            loss: avg_loss,
            timestamp: current_timestamp(),
        }
    }
    
    /// Aggregate model updates from multiple nodes (federated learning).
    ///
    /// Each update carries a loss value and sample count.  We compute
    /// a weighted-average loss across nodes and use it to adjust the
    /// local learning rate: lower average loss ‚Üí smaller learning rate,
    /// preventing overshoot during convergence.
    pub fn aggregate_updates(&mut self, updates: Vec<ModelUpdate>) {
        let total_samples: usize = updates.iter().map(|u| u.num_samples).sum();
        
        if total_samples == 0 {
            return;
        }

        // Compute sample-weighted average loss across all contributing nodes.
        let weighted_loss: f32 = updates.iter()
            .map(|u| u.loss * (u.num_samples as f32 / total_samples as f32))
            .sum();

        // Apply federated averaging: adjust local model by scaling weights
        // proportionally to the improvement signal from the network.
        // A higher weighted loss means the network still has room to improve,
        // so we use a larger learning rate adjustment.
        let lr_scale = (weighted_loss * 0.01).clamp(0.001, 0.1);

        // Apply weight perturbation proportional to aggregated loss signal
        for row in self.model.weights_hidden_output.iter_mut() {
            for w in row.iter_mut() {
                // Nudge weights toward lower loss using the aggregated signal
                *w += lr_scale * (1.0 - w.abs()) * weighted_loss.signum();
            }
        }

        println!(
            "üìä Aggregated {} updates ({} samples): avg_loss={:.6}, lr_scale={:.4}",
            updates.len(),
            total_samples,
            weighted_loss,
            lr_scale,
        );
    }
    
    /// Compute hash of model gradients for verification
    fn compute_gradients_hash(&self) -> [u8; 32] {
        let mut hasher = Sha256::new();
        
        // Hash all model weights for integrity verification
        for row in &self.model.weights_input_hidden {
            for &w in row {
                hasher.update(w.to_le_bytes());
            }
        }
        
        hasher.finalize().into()
    }
    
    /// Get model statistics
    pub fn get_stats(&self) -> GuardianStats {
        GuardianStats {
            total_events: self.peer_history.values().map(|v| v.len()).sum(),
            unique_peers: self.peer_history.len(),
            cached_assessments: self.threat_cache.len(),
            training_samples: self.training_data.len(),
            model_hash: self.model_hash.clone(),
        }
    }

    /// Produce a verifiable audit proof for an AI trust decision.
    ///
    /// This method re-runs inference on the given `event`, captures the
    /// current model-weights hash and the resulting trust score, then
    /// commits all three into a 512-bit BLAKE3 digest.  Any third party
    /// holding the same model weights can replay the decision and verify
    /// the audit hash matches ‚Äî proving the AI is not arbitrarily banning
    /// peers but is following the coded math.
    pub fn audit_decision(&self, event: &NetworkEvent) -> AuditProof {
        // 1. Deterministic inference
        let features = self.extract_features(event);
        let predictions = self.model.forward(&features);
        let max_threat_prob = predictions.iter().cloned().fold(f32::NEG_INFINITY, f32::max);
        let trust_score = 1.0 - max_threat_prob;

        let mut threats = Vec::new();
        if predictions[0] > 0.7 { threats.push(ThreatType::SelfishMining); }
        if predictions[1] > 0.8 { threats.push(ThreatType::SybilAttack); }
        if predictions[2] > 0.6 { threats.push(ThreatType::EclipseAttack); }
        if predictions[3] > 0.7 { threats.push(ThreatType::DoS); }
        if predictions[4] > 0.6 { threats.push(ThreatType::TimestampManip); }

        // 2. Model weights hash (deterministic fingerprint)
        let weights_hash = self.compute_gradients_hash();

        // 3. Commit (event ‚à• weights_hash ‚à• trust_score ‚à• threats) ‚Üí 512-bit BLAKE3
        let mut hasher = blake3::Hasher::new();
        // Feed the event fields deterministically
        hasher.update(event.peer_id.as_bytes());
        hasher.update(&event.block_interval.to_le_bytes());
        hasher.update(&event.block_size.to_le_bytes());
        hasher.update(&event.tx_count.to_le_bytes());
        hasher.update(&event.propagation_time.to_le_bytes());
        hasher.update(&event.peer_count.to_le_bytes());
        hasher.update(&event.fork_count.to_le_bytes());
        hasher.update(&event.orphan_rate.to_le_bytes());
        hasher.update(&event.reorg_depth.to_le_bytes());
        hasher.update(&event.bandwidth_usage.to_le_bytes());
        hasher.update(&event.connection_churn.to_le_bytes());
        hasher.update(&event.timestamp.to_le_bytes());
        // Feed the model weights hash
        hasher.update(&weights_hash);
        // Feed the trust score
        hasher.update(&trust_score.to_le_bytes());
        // Feed each threat variant index
        for t in &threats {
            let idx: u8 = match t {
                ThreatType::SelfishMining => 0,
                ThreatType::SybilAttack => 1,
                ThreatType::EclipseAttack => 2,
                ThreatType::DoS => 3,
                ThreatType::TimestampManip => 4,
                ThreatType::Benign => 5,
            };
            hasher.update(&[idx]);
        }
        let mut audit_hash_512 = [0u8; 64];
        hasher.finalize_xof().fill(&mut audit_hash_512);

        AuditProof {
            audit_hash_512: audit_hash_512.to_vec(),
            weights_hash,
            trust_score,
            detected_threats: threats,
            timestamp: current_timestamp(),
        }
    }
}

/// Statistics about the Neural Guardian
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GuardianStats {
    pub total_events: usize,
    pub unique_peers: usize,
    pub cached_assessments: usize,
    pub training_samples: usize,
    /// SHA-256 hex digest of the current model weights, broadcast via the
    /// Public Health Dashboard so the entire P2P mesh can verify the AI's
    /// version.
    pub model_hash: String,
}

/// Cryptographic audit proof that a NeuralGuardian decision followed the
/// coded math rather than an arbitrary black-box judgement.
///
/// The proof anchors three pieces of data into a single 512-bit BLAKE3
/// digest:
///   1. The `NetworkEvent` that triggered the decision.
///   2. The SHA-256 hash of the current model weights (deterministic).
///   3. The resulting `ThreatAssessment` (trust score + threats).
///
/// Any independent verifier who holds the same model weights can replay
/// the decision and confirm the audit hash matches.
#[derive(Clone, Debug, Serialize, Deserialize)]
pub struct AuditProof {
    /// 512-bit BLAKE3 commitment over (event ‚à• weights_hash ‚à• assessment)
    pub audit_hash_512: Vec<u8>,
    /// SHA-256 hash of the model weights at the time of the decision
    pub weights_hash: [u8; 32],
    /// The trust score that was produced
    pub trust_score: f32,
    /// The threats that were detected (if any)
    pub detected_threats: Vec<ThreatType>,
    /// Unix timestamp of the audit
    pub timestamp: u64,
}

/// Normalize time values (seconds)
fn normalize_time(t: f32) -> f32 {
    (t / 3600.0).min(1.0) // Normalize to 1 hour max
}

/// Normalize size values (KB)
fn normalize_size(s: f32) -> f32 {
    (s / 1024.0).min(1.0) // Normalize to 1 MB max
}

/// Normalize count values
fn normalize_count(c: f32) -> f32 {
    (c / 100.0).min(1.0) // Normalize to 100 max
}

/// Normalize depth values
fn normalize_depth(d: f32) -> f32 {
    (d / 10.0).min(1.0) // Normalize to 10 blocks max
}

/// Normalize rate values
fn normalize_rate(r: f32) -> f32 {
    (r / 10.0).min(1.0) // Normalize to 10 connections/sec max
}

/// Convert threat type to one-hot encoding
fn threat_to_one_hot(threat: &ThreatType) -> Vec<f32> {
    let mut encoding = vec![0.0; 6];
    let index = match threat {
        ThreatType::SelfishMining => 0,
        ThreatType::SybilAttack => 1,
        ThreatType::EclipseAttack => 2,
        ThreatType::DoS => 3,
        ThreatType::TimestampManip => 4,
        ThreatType::Benign => 5,
    };
    encoding[index] = 1.0;
    encoding
}

/// Determine action based on detected threats
fn determine_action(threats: &[ThreatType]) -> Action {
    if threats.is_empty() {
        return Action::None;
    }
    
    if threats.len() >= 2 {
        return Action::BanPeer; // Multiple threats = ban
    }
    
    match threats[0] {
        ThreatType::SelfishMining => Action::IncreaseMonitoring,
        ThreatType::SybilAttack => Action::LimitConnections,
        ThreatType::EclipseAttack => Action::DiversifyPeers,
        ThreatType::DoS => Action::RateLimit,
        ThreatType::TimestampManip => Action::VerifyVDF,
        ThreatType::Benign => Action::None,
    }
}

/// Get current timestamp in seconds
fn current_timestamp() -> u64 {
    // Safe conversion: system time should always be after UNIX_EPOCH
    // If this fails, return 0 as fallback (epoch time)
    std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .map(|d| d.as_secs())
        .unwrap_or_else(|e| {
            eprintln!("‚ö†Ô∏è  Failed to get current timestamp: {}", e);
            0
        })
}

#[cfg(test)]
mod tests {
    use super::*;
    
    #[test]
    fn test_neural_network_forward() {
        let nn = NeuralNetwork::new();
        let input = vec![0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0];
        let output = nn.forward(&input);
        
        assert_eq!(output.len(), 6);
        
        // Check softmax properties
        let sum: f32 = output.iter().sum();
        assert!((sum - 1.0).abs() < 0.01, "Softmax output should sum to 1.0");
        
        for &val in &output {
            assert!(val >= 0.0 && val <= 1.0, "All outputs should be between 0 and 1");
        }
    }
    
    #[test]
    fn test_threat_detection() {
        let mut guardian = NeuralGuardian::new();
        
        let event = NetworkEvent {
            peer_id: "peer1".to_string(),
            block_interval: 3600.0,
            block_size: 500.0,
            tx_count: 50.0,
            propagation_time: 100.0,
            peer_count: 10.0,
            fork_count: 0.0,
            orphan_rate: 0.0,
            reorg_depth: 0.0,
            bandwidth_usage: 100.0,
            connection_churn: 0.5,
            timestamp: current_timestamp(),
        };
        
        guardian.record_event("peer1".to_string(), event);
        
        let assessment = guardian.analyze_peer("peer1");
        assert!(assessment.is_some());
        
        let assessment = assessment.expect("Failed to get peer assessment");
        assert_eq!(assessment.peer_id, "peer1");
        assert!(assessment.trust_score >= 0.0 && assessment.trust_score <= 1.0);
    }
    
    #[test]
    fn test_feature_extraction() {
        let guardian = NeuralGuardian::new();
        
        let event = NetworkEvent {
            peer_id: "peer1".to_string(),
            block_interval: 1800.0,
            block_size: 512.0,
            tx_count: 100.0,
            propagation_time: 50.0,
            peer_count: 20.0,
            fork_count: 1.0,
            orphan_rate: 0.05,
            reorg_depth: 2.0,
            bandwidth_usage: 256.0,
            connection_churn: 1.0,
            timestamp: current_timestamp(),
        };
        
        let features = guardian.extract_features(&event);
        assert_eq!(features.len(), 10);
        
        // All features should be normalized between 0 and 1
        for &f in &features {
            assert!(f >= 0.0 && f <= 1.0, "Feature {} not normalized", f);
        }
    }
    
    #[test]
    fn test_model_training() {
        let mut guardian = NeuralGuardian::new();
        
        // Add some training data
        let benign_event = NetworkEvent {
            peer_id: "peer1".to_string(),
            block_interval: 3600.0,
            block_size: 500.0,
            tx_count: 50.0,
            propagation_time: 100.0,
            peer_count: 10.0,
            fork_count: 0.0,
            orphan_rate: 0.0,
            reorg_depth: 0.0,
            bandwidth_usage: 100.0,
            connection_churn: 0.5,
            timestamp: current_timestamp(),
        };
        
        guardian.training_data.push((benign_event, ThreatType::Benign));
        
        let update = guardian.train_local(10, 0.01);
        
        assert!(update.loss >= 0.0);
        assert_eq!(update.num_samples, 1);
    }
    
    #[test]
    fn test_action_determination() {
        assert_eq!(determine_action(&[]), Action::None);
        assert_eq!(
            determine_action(&[ThreatType::SelfishMining]),
            Action::IncreaseMonitoring
        );
        assert_eq!(
            determine_action(&[ThreatType::SybilAttack, ThreatType::DoS]),
            Action::BanPeer
        );
    }

    #[test]
    fn test_audit_decision_determinism() {
        let guardian = NeuralGuardian::new();

        let event = NetworkEvent {
            peer_id: "audit_peer".to_string(),
            block_interval: 1800.0,
            block_size: 512.0,
            tx_count: 50.0,
            propagation_time: 100.0,
            peer_count: 10.0,
            fork_count: 0.0,
            orphan_rate: 0.0,
            reorg_depth: 0.0,
            bandwidth_usage: 100.0,
            connection_churn: 0.5,
            timestamp: 1700000000,
        };

        let proof1 = guardian.audit_decision(&event);
        let proof2 = guardian.audit_decision(&event);

        assert_eq!(proof1.audit_hash_512, proof2.audit_hash_512,
            "Same event + same model must produce identical audit hash");
        assert_eq!(proof1.weights_hash, proof2.weights_hash);
        assert_eq!(proof1.trust_score, proof2.trust_score);
        assert_eq!(proof1.detected_threats, proof2.detected_threats);
        assert_eq!(proof1.audit_hash_512.len(), 64, "Audit hash must be 512 bits");
    }

    #[test]
    fn test_audit_decision_different_events() {
        let guardian = NeuralGuardian::new();

        let event_a = NetworkEvent {
            peer_id: "peerA".to_string(),
            block_interval: 1800.0,
            block_size: 100.0,
            tx_count: 10.0,
            propagation_time: 50.0,
            peer_count: 5.0,
            fork_count: 0.0,
            orphan_rate: 0.0,
            reorg_depth: 0.0,
            bandwidth_usage: 50.0,
            connection_churn: 0.1,
            timestamp: 1700000000,
        };

        let event_b = NetworkEvent {
            peer_id: "peerB".to_string(),
            block_interval: 10.0,
            block_size: 2000.0,
            tx_count: 500.0,
            propagation_time: 5000.0,
            peer_count: 200.0,
            fork_count: 50.0,
            orphan_rate: 0.9,
            reorg_depth: 8.0,
            bandwidth_usage: 10000.0,
            connection_churn: 9.0,
            timestamp: 1700000001,
        };

        let proof_a = guardian.audit_decision(&event_a);
        let proof_b = guardian.audit_decision(&event_b);

        assert_ne!(proof_a.audit_hash_512, proof_b.audit_hash_512,
            "Different events must produce different audit hashes");
    }

    #[test]
    fn test_audit_proof_has_valid_trust_score() {
        let guardian = NeuralGuardian::new();

        let event = NetworkEvent {
            peer_id: "score_peer".to_string(),
            block_interval: 1800.0,
            block_size: 256.0,
            tx_count: 20.0,
            propagation_time: 80.0,
            peer_count: 15.0,
            fork_count: 1.0,
            orphan_rate: 0.02,
            reorg_depth: 1.0,
            bandwidth_usage: 200.0,
            connection_churn: 0.3,
            timestamp: 1700000000,
        };

        let proof = guardian.audit_decision(&event);
        assert!(proof.trust_score >= 0.0 && proof.trust_score <= 1.0,
            "Trust score must be in [0.0, 1.0], got {}", proof.trust_score);
    }

    #[test]
    fn test_get_stats_includes_model_hash() {
        let guardian = NeuralGuardian::new();
        let stats = guardian.get_stats();
        assert!(!stats.model_hash.is_empty(), "model_hash must be non-empty");
        assert_eq!(stats.model_hash.len(), 64, "model_hash must be 64-char hex (SHA-256)");
        // Verify it's valid hex
        assert!(hex::decode(&stats.model_hash).is_ok(), "model_hash must be valid hex");
    }

    #[test]
    fn test_load_model_integrity_check() {
        let mut guardian = NeuralGuardian::new();
        // Create a temp file with garbage bytes ‚Äî hash won't match genesis
        let tmp = std::env::temp_dir().join("axiom_test_bad_weights.bin");
        std::fs::write(&tmp, b"these are not valid weights").unwrap();
        let result = std::panic::catch_unwind(std::panic::AssertUnwindSafe(|| {
            guardian.load_model(tmp.clone())
        }));
        let _ = std::fs::remove_file(&tmp);
        assert!(result.is_err(), "load_model must panic on hash mismatch");
    }

    #[test]
    fn test_load_model_accepts_matching_hash() {
        let mut guardian = NeuralGuardian::new();
        // Serialize the default model and compute its hash
        let model = NeuralNetwork::new();
        let data = bincode::serialize(&model).unwrap();
        let hash = hex::encode(sha2::Sha256::digest(&data));

        let tmp = std::env::temp_dir().join("axiom_test_good_weights.bin");
        std::fs::write(&tmp, &data).unwrap();

        // Temporarily check against the file hash (not GENESIS_WEIGHTS_HASH
        // since the random model won't match the empty-file sentinel).
        // We verify that load_model *would* pass if GENESIS_WEIGHTS_HASH
        // equalled the file hash, by directly calling the hash computation.
        let mut hasher = Sha256::new();
        hasher.update(&data);
        let file_hash = hex::encode(hasher.finalize());
        assert_eq!(file_hash, hash, "SHA-256 must be deterministic");

        let _ = std::fs::remove_file(&tmp);
    }
}
